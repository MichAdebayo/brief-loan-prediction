{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('.csv/cleaned_data.csv', index_col = 0)\n",
    "xgb_data = df.copy()\n",
    "pd.set_option('display.max_column', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_data.replace(np.inf, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_data['NewExist_Encoded'] = xgb_data['NewExist'].map({1.0 : 1, 2.0: 2, np.nan: 0})\n",
    "xgb_data['UrbanRural_Encoded'] = xgb_data['UrbanRural'].map({1.0 : 1, 2.0: 2, 0.0: 0}).fillna(0).astype('int64')\n",
    "xgb_data['MIS_Status_Encoded'] = xgb_data['MIS_Status'].map({'CHGOFF': 0, 'PIF': 1})\n",
    "xgb_data['RevLineCr_Encoded'] = xgb_data['RevLineCr'].map({'N': 1, 'Y': 2}).fillna(0).astype('int64')\n",
    "xgb_data['LowDoc_Encoded'] = xgb_data['LowDoc'].map({'N': 0, 'Y': 1})\n",
    "xgb_data['FranchiseCode_Encoded'] = xgb_data['FranchiseCode_Encoded'].map({'No': 0, 'Yes': 1}).astype('int64')\n",
    "xgb_data['RealEstate_Backed'] = xgb_data['RealEstate_Backed'].map({'No': 0, 'Yes': 1}).astype('int64')\n",
    "xgb_data['CreateJob_Encoded'] = xgb_data['CreateJob'].apply(lambda x: 1 if x > 0 else 0)\n",
    "xgb_data['RetainedJob_Encoded'] = xgb_data['RetainedJob'].apply(lambda x: 1 if x > 0 else 0)\n",
    "xgb_data['State'] = xgb_data['State'].astype('category')\n",
    "xgb_data['StateRisk'] = xgb_data['StateRisk'].astype('category')\n",
    "xgb_data['Region'] = xgb_data['Region'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_data['NewExist_Encoded'] = xgb_data['NewExist'].map({1.0 : 1, 2.0: 2}, na_action='ignore')\n",
    "# xgb_data['UrbanRural_Encoded'] = xgb_data['UrbanRural'].map({1.0 : 1, 2.0: 2, 0.0: 0}, na_action='ignore')\n",
    "# xgb_data['MIS_Status_Encoded'] = xgb_data['MIS_Status'].map({'CHGOFF': 0, 'PIF': 1})\n",
    "# xgb_data['RevLineCr_Encoded'] = xgb_data['RevLineCr'].map({'N': 1, 'Y': 2}, na_action='ignore')\n",
    "# xgb_data['LowDoc_Encoded'] = xgb_data['LowDoc'].map({'N': 0, 'Y': 1})\n",
    "# xgb_data['FranchiseCode_Encoded'] = xgb_data['FranchiseCode_Encoded'].map({'No': 0, 'Yes': 1}).astype('int64')\n",
    "# xgb_data['RealEstate_Backed'] = xgb_data['RealEstate_Backed'].map({'No': 0, 'Yes': 1}).astype('int64')\n",
    "# xgb_data['CreateJob_Encoded'] = xgb_data['CreateJob'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# xgb_data['RetainedJob_Encoded'] = xgb_data['RetainedJob'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# xgb_data['State'] = xgb_data['State'].astype('category')\n",
    "# xgb_data['StateRisk'] = xgb_data['StateRisk'].astype('category')\n",
    "# xgb_data['Region'] = xgb_data['Region'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_data.loc[:, 'EmployeeLoanRatio'] = xgb_data.apply(\n",
    "    lambda row: round(row['GrAppv']) if pd.isna(row['EmployeeLoanRatio']) else round(row['EmployeeLoanRatio']),\n",
    "    axis=1\n",
    ").astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_data.drop(labels=['LoanNr_ChkDgt', 'Name', 'City', 'Bank', 'BankState', 'TermDays', 'ApprovalDate', 'ApprovalFY', 'Zip', 'DisbursementDate', \n",
    "                      'DisbursementGross','NewExist', 'RetainedJob', 'LowDoc' ,'UrbanRural', 'RevLineCr', 'ChgOffDate',\n",
    "                      'BalanceGross', 'MIS_Status', 'ChgOffPrinGr', 'SBA_Appv', 'Industry',\n",
    "                       'ApprovalDateYear', 'ChgOffDateYear', 'ApprovalDateMonth', 'DisbursementDateYear',\n",
    "                       'LoanDateEnd'], axis=1, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dummies\n",
    "# dummy_cols = pd.get_dummies(xgb_data[['Region', 'StateRisk']])  \n",
    "\n",
    "# # Convert dummies to int\n",
    "# dummy_cols = dummy_cols.astype(int)  \n",
    "\n",
    "# # Merge back to original DataFrame\n",
    "# xgb_data = pd.concat([xgb_data, dummy_cols], axis=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_data.drop(['Region', 'StateRisk'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_features = ['GrAppv', 'CreateJob', 'Term', 'NoEmp', 'NAICS', 'EmployeeLoanRatio', 'FranchiseCode',]\n",
    "categorical_features = ['FranchiseCode_Encoded', 'RealEstate_Backed','NAICS_class_code',\n",
    "                        'Recession', 'NewExist_Encoded', 'UrbanRural_Encoded', 'RevLineCr_Encoded',\n",
    "                        'LowDoc_Encoded', 'CreateJob_Encoded', 'Region',\n",
    "                        'StateRisk', 'State']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numerics = scaler.fit_transform(xgb_data[numerical_features])\n",
    "\n",
    "xgb_data[numerical_features] = scaled_numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Define the target columns\n",
    "target_columns = ['MIS_Status_Encoded', 'State', 'Region', 'StateRisk']\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = xgb_data.drop(columns=target_columns).reset_index(drop=True)  # Drop target and reset index\n",
    "y = xgb_data[target_columns].reset_index(drop=True)  # Store target separately and reset index\n",
    "\n",
    "# Apply PolynomialFeatures\n",
    "polyfit = PolynomialFeatures(degree=2) \n",
    "X_poly = polyfit.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame with feature names\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=polyfit.get_feature_names_out(X.columns))\n",
    "\n",
    "# Concatenate transformed features and target\n",
    "xgb_data_polyfit_df = pd.concat([X_poly_df, y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features and target\n",
    "X = xgb_data.drop(columns=['MIS_Status_Encoded'])\n",
    "y = xgb_data['MIS_Status_Encoded']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # \"xgb__booster\": ['dart', 'gbtree'],\n",
    "    # \"xgb__device\" : ['cuda'],\n",
    "    # 'xgb__learning_rate': [0.01, 0.05, 0.1, 0.2, 0.8],  # Step size shrinkage\n",
    "    #'xgb__max_depth': [3, 5, 7, 12],  # Maximum depth of trees\n",
    "    # 'xgb__min_child_weight': [3, 5, 7],  # Minimum sum of weights required in a child\n",
    "    # 'xgb__gamma': [0, 0.1, 0.2],  # Minimum loss reduction for further partitioning\n",
    "    # 'xgb__subsample': [0.6, 0.8, 1.0],  # Fraction of samples per tree\n",
    "    # 'xgb__reg_alpha': [0, 0.01, 0.1, 1],  # L1 regularization\n",
    "    # 'xgb__scale_pos_weight': [2, 5, 10],  # Balance classes (useful for imbalanced datasets)\n",
    "    # 'xgb__objective': ['binary:logistic'],  # Binary classification objective\n",
    "    # 'xgb__eval_metric': ['logloss'],  # Evaluation metric\n",
    "    # 'xgb__random_state': [42]  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaeladebayo/Documents/Simplon/brief_projects/loan_prediction/.venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [13:32:44] WARNING: /Users/runner/work/xgboost/xgboost/src/context.cc:196: XGBoost is not compiled with CUDA support.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.95\n",
      "Best parameters saved to CSV successfully!\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Define model\n",
    "model = xgb.XGBClassifier(booster= 'gbtree', enable_categorical=True,\n",
    "                          device='cuda', objective='binary:logistic',\n",
    "                          eval_metric= 'logloss',\n",
    "                          subsample= 0.8,\n",
    "                          gamma= 4,\n",
    "                          colsample_bytree=0.7,\n",
    "                          max_depth= 25,\n",
    "                          reg_lambda= 0.1,\n",
    "                          reg_alpha= 10,\n",
    "                          n_estimators= 800,\n",
    "                          learning_rate=0.27777)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_score = model.score(X_train, y_train)\n",
    "\n",
    "print(\"Training Score:\", round(train_score, 2))\n",
    "\n",
    "# Store model parameters in a DataFrame\n",
    "best_xgb_params = model.get_params()  # Get model's hyperparameters\n",
    "best_xgb_params_df = pd.DataFrame([best_xgb_params])\n",
    "\n",
    "# Save to CSV\n",
    "best_xgb_params_df.to_csv(\"best_xgb_params.csv\", index=False)\n",
    "\n",
    "print(\"Best parameters saved to CSV successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83     31564\n",
      "           1       0.96      0.97      0.96    147922\n",
      "\n",
      "    accuracy                           0.94    179486\n",
      "   macro avg       0.91      0.89      0.90    179486\n",
      "weighted avg       0.94      0.94      0.94    179486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions\n",
    "xgb_y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "xgb_classification_report = classification_report(y_test, xgb_y_pred)\n",
    "\n",
    "print(xgb_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 591687, number of negative: 126255\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1734\n",
      "[LightGBM] [Info] Number of data points in the train set: 717942, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824143 -> initscore=1.544674\n",
      "[LightGBM] [Info] Start training from score 1.544674\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "Training Score: 0.96\n",
      "Best parameters saved to CSV successfully!\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Define model\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type= 'gbdt',\n",
    "                            n_jobs = 7,\n",
    "                            bagging_fraction=0.8, \n",
    "                            bagging_freq=1,\n",
    "                            feature_fraction=0.8, \n",
    "                            importance_type='gain',\n",
    "                            min_child_weight=0.1, \n",
    "                            min_split_gain=0.1,\n",
    "                            objective='binary',\n",
    "                            subsample=1.0,\n",
    "                            learning_rate=0.2777,\n",
    "                            n_estimators= 800,\n",
    "                            )\n",
    "\n",
    "# \"lgb__boosting_type\": ['gbdt',\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "lgb_train_score = lgb_model.score(X_train, y_train)\n",
    "\n",
    "print(\"Training Score:\", round(lgb_train_score, 2))\n",
    "\n",
    "# Store model parameters in a DataFrame\n",
    "best_lgb_params = lgb_model.get_params()  # Get model's hyperparameters\n",
    "best_lgb_params_df = pd.DataFrame([best_lgb_params])\n",
    "\n",
    "# Save to CSV\n",
    "best_lgb_params_df.to_csv(\"best_lgb_params.csv\", index=False)\n",
    "\n",
    "print(\"Best parameters saved to CSV successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84     31564\n",
      "           1       0.96      0.97      0.97    147922\n",
      "\n",
      "    accuracy                           0.94    179486\n",
      "   macro avg       0.91      0.90      0.90    179486\n",
      "weighted avg       0.94      0.94      0.94    179486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions\n",
    "lgb_y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "lgb_classification_report = classification_report(y_test, lgb_y_pred)\n",
    "\n",
    "print(lgb_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4238619\ttotal: 257ms\tremaining: 4.88s\n",
      "1:\tlearn: 0.3292202\ttotal: 452ms\tremaining: 4.07s\n",
      "2:\tlearn: 0.2836503\ttotal: 644ms\tremaining: 3.65s\n",
      "3:\tlearn: 0.2579911\ttotal: 817ms\tremaining: 3.27s\n",
      "4:\tlearn: 0.2388670\ttotal: 956ms\tremaining: 2.87s\n",
      "5:\tlearn: 0.2215477\ttotal: 1.13s\tremaining: 2.63s\n",
      "6:\tlearn: 0.2150707\ttotal: 1.3s\tremaining: 2.42s\n",
      "7:\tlearn: 0.2083435\ttotal: 1.45s\tremaining: 2.18s\n",
      "8:\tlearn: 0.2047586\ttotal: 1.63s\tremaining: 1.99s\n",
      "9:\tlearn: 0.2010778\ttotal: 1.81s\tremaining: 1.81s\n",
      "10:\tlearn: 0.1956705\ttotal: 2s\tremaining: 1.63s\n",
      "11:\tlearn: 0.1909566\ttotal: 2.18s\tremaining: 1.45s\n",
      "12:\tlearn: 0.1880429\ttotal: 2.35s\tremaining: 1.27s\n",
      "13:\tlearn: 0.1861644\ttotal: 2.55s\tremaining: 1.09s\n",
      "14:\tlearn: 0.1842904\ttotal: 2.73s\tremaining: 911ms\n",
      "15:\tlearn: 0.1813587\ttotal: 2.88s\tremaining: 720ms\n",
      "16:\tlearn: 0.1800970\ttotal: 3.07s\tremaining: 541ms\n",
      "17:\tlearn: 0.1776264\ttotal: 3.23s\tremaining: 359ms\n",
      "18:\tlearn: 0.1758857\ttotal: 3.4s\tremaining: 179ms\n",
      "19:\tlearn: 0.1750090\ttotal: 3.58s\tremaining: 0us\n",
      "Training Score: 0.94\n",
      "Best parameters saved to CSV successfully!\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "# Define model\n",
    "cb_model = cb.CatBoostClassifier(iterations= 20,\n",
    "                                max_depth=10,\n",
    "                                learning_rate=0.2777,\n",
    "                                l2_leaf_reg= 0.1,\n",
    "                            )\n",
    "\n",
    "categorical_features_indices = [X_train.columns.get_loc(col) for col in X_train.select_dtypes(include=['category']).columns]\n",
    "cb_model.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "\n",
    "cb_train_score = cb_model.score(X_train, y_train)\n",
    "\n",
    "print(\"Training Score:\", round(cb_train_score, 2))\n",
    "\n",
    "# Store model parameters in a DataFrame\n",
    "best_cb_params = cb_model.get_params()  # Get model's hyperparameters\n",
    "best_cb_params_df = pd.DataFrame([best_cb_params])\n",
    "\n",
    "# Save to CSV\n",
    "best_cb_params_df.to_csv(\"best_cb_params.csv\", index=False)\n",
    "\n",
    "print(\"Best parameters saved to CSV successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.77      0.81     31564\n",
      "           1       0.95      0.97      0.96    147922\n",
      "\n",
      "    accuracy                           0.93    179486\n",
      "   macro avg       0.90      0.87      0.88    179486\n",
      "weighted avg       0.93      0.93      0.93    179486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions\n",
    "cb_y_pred = cb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "cb_classification_report = classification_report(y_test, cb_y_pred)\n",
    "\n",
    "print(cb_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# model = xgb.XGBClassifier(booster='gbtree', enable_categorical=True,\n",
    "#                           device='cuda',\n",
    "#                           objective='binary:logistic')\n",
    "\n",
    "# # Define search space for hyperparameters\n",
    "# param_dist = {\n",
    "#     \"n_estimators\": np.arange(100, 501, 100),  # 100 to 1000, step 100\n",
    "#     \"max_depth\": np.arange(3, 16, 2),  # 3 to 15, step 2\n",
    "#     \"learning_rate\": np.linspace(0.01, 0.3, 5),  # 10 values between 0.01 and 0.3\n",
    "#     \"gamma\": np.linspace(0, 5, 4),  # 6 values between 0 and 5\n",
    "#     \"reg_lambda\": np.logspace(-3, 2, 4),  # Regularization term λ\n",
    "#     \"reg_alpha\": np.logspace(-3, 2, 4),  # Regularization term α\n",
    "# }\n",
    "\n",
    "# # Perform randomized search\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     model, param_distributions=param_dist, \n",
    "#     n_iter=5,  # Number of random parameter combinations to try\n",
    "#     scoring=\"accuracy\", \n",
    "#     cv=4,  # 5-fold cross-validation\n",
    "#     verbose=1, \n",
    "#     n_jobs=-1,  # Use all CPU cores\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit RandomizedSearchCV\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get best parameters\n",
    "# best_xgb_params = random_search.best_params_\n",
    "# best_xgb_params_df = pd.DataFrame([best_xgb_params])\n",
    "\n",
    "# # Save best parameters to CSV\n",
    "# best_xgb_params_df.to_csv(\"best_xgb_params.csv\", index=False)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Best Parameters:\", best_xgb_params)\n",
    "# print(\"Best Score:\", round(random_search.best_score_, 2))\n",
    "# print(\"Best parameters saved to CSV successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# t_model = random_search.best_estimator_\n",
    "\n",
    "# # Make predictions\n",
    "# xgb_y_pred = t_model.predict(X_test)\n",
    "\n",
    "# # Calculate the accuracy\n",
    "# xgb_classification_report = classification_report(y_test, xgb_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xgb_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # Evaluate model using cross-validation\n",
    "# xgb_roc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "# print(f'ROC-AUC: {xgb_roc_scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_data['NewExist_Encoded'] = xgb_data['NewExist'].map({1.0 : 1, 2.0: 2, np.nan: 0})\n",
    "# xgb_data['UrbanRural_Encoded'] = xgb_data['UrbanRural'].map({1.0 : 1, 2.0: 2, 0.0: 0}).fillna(0).astype('int64')\n",
    "# xgb_data['MIS_Status_Encoded'] = xgb_data['MIS_Status'].map({'CHGOFF': 0, 'PIF': 1})\n",
    "# xgb_data['RevLineCr_Encoded'] = xgb_data['RevLineCr'].map({'N': 1, 'Y': 2}).fillna(0).astype('int64')\n",
    "# xgb_data['LowDoc_Encoded'] = xgb_data['LowDoc'].map({'N': 0, 'Y': 1})\n",
    "# xgb_data['FranchiseCode_Encoded'] = xgb_data['FranchiseCode_Encoded'].map({'No': 0, 'Yes': 1}).astype('int64')\n",
    "# xgb_data['RealEstate_Backed'] = xgb_data['RealEstate_Backed'].map({'No': 0, 'Yes': 1}).astype('int64')\n",
    "# xgb_data['CreateJob_Encoded'] = xgb_data['CreateJob'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# xgb_data['RetainedJob_Encoded'] = xgb_data['RetainedJob'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encode categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_multi = xgb_data[['NAICS', 'Term', 'NoEmp',\n",
    "#        'CreateJob', 'RetainedJob', 'FranchiseCode', 'GrAppv',\n",
    "#        'NAICS_class_code', 'FranchiseCode_Encoded', 'RealEstate_Backed',\n",
    "#        'TermDays', 'Recession', 'EmployeeLoanRatio', 'NewExist_Encoded',\n",
    "#        'UrbanRural_Encoded', 'MIS_Status_Encoded', 'RevLineCr_Encoded',\n",
    "#        'LowDoc_Encoded', 'CreateJob_Encoded', 'RetainedJob_Encoded',\n",
    "#        'Region_Northern', 'Region_Southern', 'Region_Western', 'StateRisk_Low',\n",
    "#        'StateRisk_Medium', 'State_AL', 'State_AR', 'State_AZ', 'State_CA',\n",
    "#        'State_CO', 'State_CT', 'State_DC', 'State_DE', 'State_FL', 'State_GA',\n",
    "#        'State_HI', 'State_IA', 'State_ID', 'State_IL', 'State_IN', 'State_KS',\n",
    "#        'State_KY', 'State_LA', 'State_MA', 'State_MD', 'State_ME', 'State_MI',\n",
    "#        'State_MN', 'State_MO', 'State_MS', 'State_MT', 'State_NC', 'State_ND',\n",
    "#        'State_NE', 'State_NH', 'State_NJ', 'State_NM', 'State_NV', 'State_NY',\n",
    "#        'State_OH', 'State_OK', 'State_OR', 'State_PA', 'State_RI', 'State_SC',\n",
    "#        'State_SD', 'State_TN', 'State_TX', 'State_UT', 'State_VA', 'State_VT',\n",
    "#        'State_WA', 'State_WI', 'State_WV', 'State_WY']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
