{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('.csv/cleaned_data.csv', index_col = 0)\n",
    "lgb_data = df.copy()\n",
    "pd.set_option('display.max_column', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_data.replace(np.inf, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_data['NewExist_Encoded'] = lgb_data['NewExist'].map({1.0 : 1, 2.0: 2, np.nan: 0})\n",
    "lgb_data['UrbanRural_Encoded'] = lgb_data['UrbanRural'].map({1.0 : 1, 2.0: 2, 0.0: 0}).fillna(0).astype('int64')\n",
    "lgb_data['MIS_Status_Encoded'] = lgb_data['MIS_Status'].map({'CHGOFF': 0, 'PIF': 1})\n",
    "lgb_data['RevLineCr_Encoded'] = lgb_data['RevLineCr'].map({'N': 1, 'Y': 2}).fillna(0).astype('int64')\n",
    "lgb_data['LowDoc_Encoded'] = lgb_data['LowDoc'].map({'N': 0, 'Y': 1})\n",
    "lgb_data['FranchiseCode_Encoded'] = lgb_data['FranchiseCode_Encoded'].map({'No': 0, 'Yes': 1}).astype('int64')\n",
    "lgb_data['RealEstate_Backed'] = lgb_data['RealEstate_Backed'].map({'No': 0, 'Yes': 1}).astype('int64')\n",
    "lgb_data['CreateJob_Encoded'] = lgb_data['CreateJob'].apply(lambda x: 1 if x > 0 else 0)\n",
    "lgb_data['RetainedJob_Encoded'] = lgb_data['RetainedJob'].apply(lambda x: 1 if x > 0 else 0)\n",
    "lgb_data['State'] = lgb_data['State'].astype('category')\n",
    "lgb_data['StateRisk'] = lgb_data['StateRisk'].astype('category')\n",
    "lgb_data['Region'] = lgb_data['Region'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_data.loc[:, 'EmployeeLoanRatio'] = lgb_data.apply(\n",
    "    lambda row: round(row['GrAppv']) if pd.isna(row['EmployeeLoanRatio']) else round(row['EmployeeLoanRatio']),\n",
    "    axis=1\n",
    ").astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_data.drop(labels=['LoanNr_ChkDgt', 'Name', 'City', 'Bank', 'BankState', 'TermDays', 'ApprovalDate', 'ApprovalFY', 'Zip', 'DisbursementDate', \n",
    "                      'DisbursementGross','NewExist', 'RetainedJob', 'LowDoc' ,'UrbanRural', 'RevLineCr', 'ChgOffDate',\n",
    "                      'BalanceGross', 'MIS_Status', 'ChgOffPrinGr', 'SBA_Appv', 'Industry',\n",
    "                       'ApprovalDateYear', 'ChgOffDateYear', 'ApprovalDateMonth', 'DisbursementDateYear',\n",
    "                       'LoanDateEnd'], axis=1, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_features = ['GrAppv', 'CreateJob', 'Term', 'NoEmp', 'NAICS',]\n",
    "categorical_features = ['FranchiseCode_Encoded', 'RealEstate_Backed','NAICS_class_code',\n",
    "                        'Recession', 'NewExist_Encoded', 'UrbanRural_Encoded', 'RevLineCr_Encoded',\n",
    "                        'LowDoc_Encoded', 'CreateJob_Encoded', 'Region_Eastern', \n",
    "                        'Region_Northern', 'Region_Southern', 'Region_Western'\n",
    "                        ]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numerics = scaler.fit_transform(lgb_data[numerical_features])\n",
    "\n",
    "lgb_data[numerical_features] = scaled_numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Define the target columns\n",
    "target_columns = ['MIS_Status_Encoded', 'State', 'Region', 'StateRisk']\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = lgb_data.drop(columns=target_columns).reset_index(drop=True)  # Drop target and reset index\n",
    "y = lgb_data[target_columns].reset_index(drop=True)  # Store target separately and reset index\n",
    "\n",
    "# Apply PolynomialFeatures\n",
    "polyfit = PolynomialFeatures(degree=2) \n",
    "X_poly = polyfit.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame with feature names\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=polyfit.get_feature_names_out(X.columns))\n",
    "\n",
    "# Concatenate transformed features and target\n",
    "lgb_data_polyfit_df = pd.concat([X_poly_df, y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features and target\n",
    "X = lgb_data.drop(columns=['MIS_Status_Encoded'])\n",
    "y = lgb_data['MIS_Status_Encoded']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 591687, number of negative: 126255\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1729\n",
      "[LightGBM] [Info] Number of data points in the train set: 717942, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824143 -> initscore=1.544674\n",
      "[LightGBM] [Info] Start training from score 1.544674\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "Training Score: 0.95\n",
      "Best parameters saved to CSV successfully!\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Define model\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type= 'dart',\n",
    "                            n_jobs = 7,\n",
    "                            bagging_fraction=0.8, \n",
    "                            bagging_freq=1,\n",
    "                            feature_fraction=0.8, \n",
    "                            importance_type='gain',\n",
    "                            min_child_weight=0.1, \n",
    "                            min_split_gain=0.1,\n",
    "                            objective='binary',\n",
    "                            subsample=1.0,\n",
    "                            learning_rate=0.2777,\n",
    "                            n_estimators= 800,\n",
    "                            )\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "lgb_train_score = lgb_model.score(X_train, y_train)\n",
    "\n",
    "print(\"Training Score:\", round(lgb_train_score, 2))\n",
    "\n",
    "# Store model parameters in a DataFrame\n",
    "best_lgb_params = lgb_model.get_params()  # Get model's hyperparameters\n",
    "best_lgb_params_df = pd.DataFrame([best_lgb_params])\n",
    "\n",
    "# Save to CSV\n",
    "best_lgb_params_df.to_csv(\"best_lgb_params_2.csv\", index=False)\n",
    "\n",
    "print(\"Best parameters saved to CSV successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84     31564\n",
      "           1       0.96      0.97      0.97    147922\n",
      "\n",
      "    accuracy                           0.94    179486\n",
      "   macro avg       0.91      0.90      0.90    179486\n",
      "weighted avg       0.94      0.94      0.94    179486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions\n",
    "lgb_y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "lgb_classification_report = classification_report(y_test, lgb_y_pred)\n",
    "\n",
    "print(lgb_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 473349, number of negative: 101004\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1735\n",
      "[LightGBM] [Info] Number of data points in the train set: 574353, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824143 -> initscore=1.544673\n",
      "[LightGBM] [Info] Start training from score 1.544673\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 473349, number of negative: 101004\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1734\n",
      "[LightGBM] [Info] Number of data points in the train set: 574353, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824143 -> initscore=1.544673\n",
      "[LightGBM] [Info] Start training from score 1.544673\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 473350, number of negative: 101004\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1733\n",
      "[LightGBM] [Info] Number of data points in the train set: 574354, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824143 -> initscore=1.544675\n",
      "[LightGBM] [Info] Start training from score 1.544675\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 473350, number of negative: 101004\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1738\n",
      "[LightGBM] [Info] Number of data points in the train set: 574354, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824143 -> initscore=1.544675\n",
      "[LightGBM] [Info] Start training from score 1.544675\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 473350, number of negative: 101004\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1735\n",
      "[LightGBM] [Info] Number of data points in the train set: 574354, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.824143 -> initscore=1.544675\n",
      "[LightGBM] [Info] Start training from score 1.544675\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "ROC-AUC: 0.976055437801679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate model using cross-validation\n",
    "lgb_roc_scores = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(f'ROC-AUC: {lgb_roc_scores.mean()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
